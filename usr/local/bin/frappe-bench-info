#!/usr/bin/env python3

import subprocess, sys, os, json, argparse, getpass, hashlib, time
import pymysql
from tabulate import tabulate
from concurrent.futures import ThreadPoolExecutor

# Ensure dependencies are installed
for pkg in ["pymysql", "tabulate"]:
    try:
        __import__(pkg)
    except ImportError:
        print(f"\033[33m{pkg} not found, installing...\033[0m")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--break-system-packages", pkg])

# ANSI Colors
CYAN = "\033[96m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
RESET = "\033[0m"

CACHE_FILE = os.path.expanduser("~/.frappe_bench_info_cache.json")
CACHE_TTL = 300  # 5 minutes


def get_package_version(pkg_name="frappe-bench-info"):
    try:
        result = subprocess.run(
            ["dpkg-query", "-W", "-f", "${Version}", pkg_name],
            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True
        )
        version = result.stdout.strip()
        return version if version else "unknown"
    except Exception:
        return "unknown"


def get_args():
    parser = argparse.ArgumentParser(description="Frappe Bench Info Tool (Optimized)")
    parser.add_argument('--db_user', default='root', help='MariaDB username (default: root)')
    parser.add_argument('--db_host', default='localhost', help='MariaDB host (default: localhost)')
    parser.add_argument('--start_path', help='Path to start scanning (default: current directory)')
    parser.add_argument('--version', action='store_true', help='Show version and exit')
    parser.add_argument('--refresh', action='store_true', help='Ignore cache and refresh results')

    args = parser.parse_args()

    if args.version:
        version = get_package_version("frappe-bench-info")
        print(f"frappe-bench-info v{version}")
        sys.exit(0)

    args.start_path = os.path.abspath(args.start_path or os.getcwd())
    args.db_password = getpass.getpass("Enter MariaDB password: ")
    return args


def is_frappe_bench(path):
    return (
        os.path.exists(os.path.join(path, "Procfile")) and
        os.path.exists(os.path.join(path, "sites")) and
        os.path.exists(os.path.join(path, "apps", "frappe"))
    )


def find_all_benches(start_path):
    """Faster bench finder: uses 'find' if available, else threaded scan."""
    try:
        result = subprocess.run(
            ["find", start_path, "-maxdepth", "4", "-type", "f", "-name", "Procfile"],
            stdout=subprocess.PIPE, text=True, check=True
        )
        benches = []
        for line in result.stdout.strip().split("\n"):
            if not line:
                continue
            bench_path = os.path.dirname(line)
            if os.path.isdir(os.path.join(bench_path, "sites")) and \
               os.path.isdir(os.path.join(bench_path, "apps", "frappe")):
                benches.append(bench_path)
        return benches
    except Exception:
        candidates = []
        for root, dirs, files in os.walk(start_path):
            if "Procfile" in files:
                candidates.append(root)

        with ThreadPoolExecutor(max_workers=8) as executor:
            results = executor.map(is_frappe_bench, candidates)

        return [candidates[i] for i, ok in enumerate(results) if ok]


def load_site_config(site_path):
    config_path = os.path.join(site_path, "site_config.json")
    if os.path.isfile(config_path):
        try:
            with open(config_path) as f:
                config = json.load(f)
                return config.get("db_name")
        except Exception as e:
            print(f"{YELLOW}Warning: Error reading {config_path}: {e}{RESET}")
    return None


def get_site_db_mapping(bench_path):
    site_db_map = {}
    sites_path = os.path.join(bench_path, "sites")
    if not os.path.isdir(sites_path):
        return site_db_map

    site_names = os.listdir(sites_path)
    site_paths = [os.path.join(sites_path, s) for s in site_names]

    with ThreadPoolExecutor(max_workers=8) as executor:
        results = executor.map(load_site_config, site_paths)

    for site, db_name in zip(site_names, results):
        if db_name:
            site_db_map[site] = {
                "db_name": db_name,
                "bench": os.path.basename(bench_path),
                "bench_path": bench_path
            }

    return site_db_map


def get_all_site_mappings(benches):
    all_sites = {}
    for bench_path in benches:
        all_sites.update(get_site_db_mapping(bench_path))
    return all_sites


def get_db_sizes(db_user, db_password, db_host, db_names):
    if not db_names:
        return {}

    try:
        conn = pymysql.connect(
            host=db_host,
            user=db_user,
            password=db_password,
            database="information_schema",
            cursorclass=pymysql.cursors.DictCursor
        )
    except pymysql.err.OperationalError:
        print(f"\033[91mError: Could not connect to MySQL. Check credentials.\033[0m")
        return {}

    placeholders = ",".join(["%s"] * len(db_names))
    query = f"""
        SELECT table_schema AS db_name,
               ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS size_mb
        FROM tables
        WHERE table_schema IN ({placeholders})
        GROUP BY table_schema
    """

    with conn.cursor() as cursor:
        cursor.execute(query, tuple(db_names))
        return {row["db_name"]: row["size_mb"] for row in cursor.fetchall()}


def load_cache(key):
    if not os.path.isfile(CACHE_FILE):
        return None
    try:
        with open(CACHE_FILE) as f:
            cache = json.load(f)
        if cache["key"] == key and time.time() - cache["timestamp"] < CACHE_TTL:
            return cache["data"]
    except Exception:
        return None
    return None


def save_cache(key, data):
    try:
        with open(CACHE_FILE, "w") as f:
            json.dump({
                "key": key,
                "timestamp": time.time(),
                "data": data
            }, f)
    except Exception:
        pass


def main():
    args = get_args()
    benches = find_all_benches(args.start_path)
    if not benches:
        print(f"\033[91mNo Frappe benches found in: {args.start_path}\033[0m")
        return

    site_map = get_all_site_mappings(benches)
    db_names = [info["db_name"] for info in site_map.values()]

    # Cache key is based on db_host + user + site DB names hash
    cache_key = hashlib.md5(
        (args.db_host + args.db_user + ",".join(sorted(db_names))).encode()
    ).hexdigest()

    cached = None if args.refresh else load_cache(cache_key)
    if cached:
        db_sizes = cached
    else:
        db_sizes = get_db_sizes(args.db_user, args.db_password, args.db_host, db_names)
        save_cache(cache_key, db_sizes)

    print("\n")
    print("FRAPPE BENCH INFO TOOL")
    print("══════════════════════")

    table_data = []
    for site, info in site_map.items():
        db = info["db_name"]
        bench = info["bench"]
        bench_path = info["bench_path"]
        size = db_sizes.get(db, 0.0)

        table_data.append([
            CYAN + site + RESET,
            GREEN + bench + RESET,
            YELLOW + db + RESET,
            f"{size:.2f}",
            bench_path
        ])

    headers = [
        CYAN + "Site Name" + RESET,
        GREEN + "Bench Name" + RESET,
        YELLOW + "DB Name" + RESET,
        "DB Size (MB)",
        "Bench Path"
    ]

    print(tabulate(table_data, headers=headers, tablefmt="grid"))


if __name__ == "__main__":
    main()
